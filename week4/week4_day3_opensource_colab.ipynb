{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9",
      "metadata": {
        "id": "4a6ab9a2-28a2-445d-8512-a0dc8d1b54e9"
      },
      "source": [
        "# Code Generator\n",
        "\n",
        "The requirement: use a Frontier model to generate high performance C++ code from Python code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0",
      "metadata": {
        "id": "d5ccb926-7b49-44a4-99ab-8ef20b5778c0"
      },
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h2 style=\"color:#f71;\">Reminder: fetch latest code</h2>\n",
        "            <span style=\"color:#f71;\">I'm continually improving these labs, adding more examples and exercises.\n",
        "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
        "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
        "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
        "            <code>conda env update --f environment.yml --prune</code><br/>\n",
        "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
        "            <code>pip install -r requirements.txt</code>\n",
        "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313",
      "metadata": {
        "id": "d90e04a2-5b8a-4fd5-9db8-27c02f033313"
      },
      "source": [
        "<table style=\"margin: 0; text-align: left;\">\n",
        "    <tr>\n",
        "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
        "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
        "        </td>\n",
        "        <td>\n",
        "            <h1 style=\"color:#900;\">Important Note</h1>\n",
        "            <span style=\"color:#900;\">\n",
        "            In this lab, I use GPT-4o and Claude-3.5-Sonnet, which are the slightly higher priced models. The costs are still low, but if you'd prefer to keep costs ultra low, please make the suggested switches to the models (3 cells down from here).\n",
        "            </span>\n",
        "        </td>\n",
        "    </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3",
      "metadata": {
        "id": "e610bf56-a46e-4aff-8de1-ab49d62b1ad3"
      },
      "outputs": [],
      "source": [
        "#! pip install gradio\n",
        "\n",
        "# imports\n",
        "\n",
        "#import os\n",
        "#import io\n",
        "#import sys\n",
        "#from dotenv import load_dotenv\n",
        "#from openai import OpenAI\n",
        "#import google.generativeai\n",
        "#import anthropic\n",
        "#from IPython.display import Markdown, display, update_display\n",
        "#import gradio as gr\n",
        "#import subprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6u_4KanLNC-",
      "metadata": {
        "id": "b6u_4KanLNC-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6896636f-923e-4a2c-9d6c-fac07828a201",
      "metadata": {
        "id": "6896636f-923e-4a2c-9d6c-fac07828a201"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. \"\n",
        "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
        "system_message += \"The C++ response needs to produce an identical output in the fastest possible time.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "bcbd55f2-1b27-41d2-80c7-40ea41e2eec9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcbd55f2-1b27-41d2-80c7-40ea41e2eec9",
        "outputId": "66350318-2841-4790-f8a4-ed019e61d61a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. The C++ response needs to produce an identical output in the fastest possible time.\n"
          ]
        }
      ],
      "source": [
        "print(system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb",
      "metadata": {
        "id": "8e7b3546-57aa-4c29-bc5d-f211970d04eb"
      },
      "outputs": [],
      "source": [
        "def user_prompt_for(python):\n",
        "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
        "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
        "    user_prompt += python\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6190659-f54c-4951-bef4-4960f8e51cc4",
      "metadata": {
        "id": "c6190659-f54c-4951-bef4-4960f8e51cc4"
      },
      "outputs": [],
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt_for(python)}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b",
      "metadata": {
        "id": "71e1ba8c-5b05-4726-a9f3-8d8c6257350b"
      },
      "outputs": [],
      "source": [
        "# write to a file called optimized.cpp\n",
        "\n",
        "def write_output(cpp):\n",
        "    code = cpp.replace(\"```cpp\",\"\").replace(\"```\",\"\")\n",
        "    with open(\"optimized.cpp\", \"w\") as f:\n",
        "        f.write(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a1cbb778-fa57-43de-b04b-ed523f396c38",
      "metadata": {
        "id": "a1cbb778-fa57-43de-b04b-ed523f396c38"
      },
      "outputs": [],
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(100_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fe1cd4b-d2c5-4303-afed-2115a3fef200",
        "outputId": "8b74fcea-fe8c-4be9-e703-1be38bf8fb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: 3.141592658589\n",
            "Execution Time: 22.617263 seconds\n"
          ]
        }
      ],
      "source": [
        "exec(pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0",
      "metadata": {
        "id": "c3b497b3-f569-420e-b92e-fb0f49957ce0"
      },
      "outputs": [],
      "source": [
        "python_hard = \"\"\"# Be careful to support large number sizes\n",
        "\n",
        "def lcg(seed, a=1664525, c=1013904223, m=2**32):\n",
        "    value = seed\n",
        "    while True:\n",
        "        value = (a * value + c) % m\n",
        "        yield value\n",
        "\n",
        "def max_subarray_sum(n, seed, min_val, max_val):\n",
        "    lcg_gen = lcg(seed)\n",
        "    random_numbers = [next(lcg_gen) % (max_val - min_val + 1) + min_val for _ in range(n)]\n",
        "    max_sum = float('-inf')\n",
        "    for i in range(n):\n",
        "        current_sum = 0\n",
        "        for j in range(i, n):\n",
        "            current_sum += random_numbers[j]\n",
        "            if current_sum > max_sum:\n",
        "                max_sum = current_sum\n",
        "    return max_sum\n",
        "\n",
        "def total_max_subarray_sum(n, initial_seed, min_val, max_val):\n",
        "    total_sum = 0\n",
        "    lcg_gen = lcg(initial_seed)\n",
        "    for _ in range(20):\n",
        "        seed = next(lcg_gen)\n",
        "        total_sum += max_subarray_sum(n, seed, min_val, max_val)\n",
        "    return total_sum\n",
        "\n",
        "# Parameters\n",
        "n = 10000         # Number of random numbers\n",
        "initial_seed = 42 # Initial seed for the LCG\n",
        "min_val = -10     # Minimum value of random numbers\n",
        "max_val = 10      # Maximum value of random numbers\n",
        "\n",
        "# Timing the function\n",
        "import time\n",
        "start_time = time.time()\n",
        "result = total_max_subarray_sum(n, initial_seed, min_val, max_val)\n",
        "end_time = time.time()\n",
        "\n",
        "print(\"Total Maximum Subarray Sum (20 runs):\", result)\n",
        "print(\"Execution Time: {:.6f} seconds\".format(end_time - start_time))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5agGi-X5N8lF",
      "metadata": {
        "id": "5agGi-X5N8lF"
      },
      "outputs": [],
      "source": [
        "!pip install -q  transformers bitsandbytes\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# HF\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "OB4Ajl4xRB29",
      "metadata": {
        "id": "OB4Ajl4xRB29"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Qwen2.5-Coder-32B-Instruct\n",
        "\n",
        "#model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct\"\n",
        "#model_name = \"Qwen/Qwen2.5-Coder-32B-Instruct\"\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
        "#model_name = \"Qwen/CodeQwen1.5-7B-Chat\"\n",
        "#model_name = \"Qwen/Qwen2.5-Coder-14B-Instruct-GGUF\"\n",
        "\n",
        "\n",
        "def create_model_and_tokenizer():\n",
        "\n",
        "    # Configure 4-bit quantization\n",
        "    bnb_config_4bit = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",  # normalized float 4\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "\n",
        "    # Configure 8-bit quantization\n",
        "    bnb_config_8bit = BitsAndBytesConfig(\n",
        "      load_in_8bit=True,\n",
        "      llm_int8_enable_fp32_cpu_offload=True,\n",
        "      llm_int8_threshold=6.0, # 10.0,\n",
        "      llm_int8_skip_modules=[\"lm_head\"],\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=\"auto\",\n",
        "        device_map=\"auto\",\n",
        "#        quantization_config=bnb_config_4bit, # <------------------------------ Quant ON/OFF\n",
        "        quantization_config=bnb_config_8bit\n",
        "        )\n",
        "\n",
        "    print(\"---------------------------------------------------\")\n",
        "    memory = model.get_memory_footprint() / 1e6\n",
        "    print(f\"Model Get Memory footprint is: =====> {memory:,.1f} MB\")\n",
        "    model\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    return model, tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4db47e9VZXTn",
      "metadata": {
        "id": "4db47e9VZXTn"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "\n",
        "def cleanup_model_resources():\n",
        "    \"\"\"\n",
        "    Comprehensive cleanup of model resources and memory\n",
        "    \"\"\"\n",
        "    # Clear CUDA cache if using GPU\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        # Optional: Reset the CUDA device\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        # Optional: Synchronize CUDA to ensure all operations are complete\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # Run garbage collection multiple times to catch circular references\n",
        "    for _ in range(3):\n",
        "        gc.collect()\n",
        "\n",
        "    # Force Python to release memory back to OS (Unix/Linux only)\n",
        "    if hasattr(os, 'malloc_trim'):\n",
        "        os.malloc_trim(0)\n",
        "\n",
        "    if 'model' in locals():\n",
        "\n",
        "        # Clear the model from GPU memory explicitly\n",
        "        if hasattr(model, 'cpu'):\n",
        "            model.cpu()\n",
        "\n",
        "        # Clear specific components\n",
        "        if hasattr(model, 'transformer'):\n",
        "            del model.transformer\n",
        "        if hasattr(model, 'embeddings'):\n",
        "            del model.embeddings\n",
        "\n",
        "        del model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "T3WFpf8h0yEU",
      "metadata": {
        "id": "T3WFpf8h0yEU"
      },
      "outputs": [],
      "source": [
        "def run(system_message: str, user_prompt: str, model, tokenizer, max_new_tokens: int = 512) -> str:\n",
        "    \"\"\"\n",
        "    Generate a response using a chat model.\n",
        "\n",
        "    Args:\n",
        "        system_message (str): The system message to set context\n",
        "        user_prompt (str): The user's input prompt\n",
        "        model: The loaded model instance\n",
        "        tokenizer: The loaded tokenizer instance\n",
        "        max_new_tokens (int): Maximum number of tokens to generate\n",
        "\n",
        "    Returns:\n",
        "        str: The generated response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ]\n",
        "\n",
        "\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        generated_ids = model.generate(\n",
        "#            **model_inputs,\n",
        "            model_inputs.input_ids,  # NEW for Qwen/coder\n",
        "            max_new_tokens=max_new_tokens\n",
        "        )\n",
        "\n",
        "        generated_ids = [\n",
        "            output_ids[len(input_ids):]\n",
        "            for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error in text generation: {str(e)}\")\n",
        "    finally:\n",
        "        # Clean up any tensors to help with memory management\n",
        "        if 'model_inputs' in locals():\n",
        "            del model_inputs\n",
        "        if 'generated_ids' in locals():\n",
        "            del generated_ids\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "# Example usage:\n",
        "# response = run(\n",
        "#     system_message=\"You are a helpful assistant\",\n",
        "#     user_prompt=\"Write a hello world program\",\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zqEeBtj_1GVn",
      "metadata": {
        "id": "zqEeBtj_1GVn"
      },
      "outputs": [],
      "source": [
        "## STEP 4:\n",
        "\n",
        "model, tokenizer = create_model_and_tokenizer()\n",
        "\n",
        "\n",
        "#cleanup_model_resources()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "python = python_hard\n",
        "\n",
        "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. \"\n",
        "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
        "system_message += \"The C++ response needs to produce an identical output in the fastest possible time.\"\n",
        "\n",
        "# possibly give hints, like:\n",
        "## Kadane's algorithm for max subarray sum\n",
        "algoritm_hint = \"Your code does not need to follow the same algorithms, so consider using other relevant algorithms in the same problem domain.\"\n",
        "#algoritm_hint += \"If you can use better algorithms please do so, and add a small comment on the algorithm used. \"\n",
        "\n",
        "user_msg = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
        "user_msg += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "user_msg += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
        "\n",
        "# Ed Hint!!\n",
        "user_msg += \"Keep implementations for random numbers identical so that results match exactly.\\n\\n\"\n",
        "user_msg += \"# You are allowed to change the algorithms if there are suitable optimizations. The only requirement is that the final result is the same.\\n\\n\"\n",
        "#user_msg += \"Be careful of errors like: shift count >= width of type [-Wshift-count-overflow]unsigned int lcg(unsigned int& seed, unsigned int a = 1664525, unsigned int c = 1013904223, unsigned int m = 1 << 32)\"\n",
        "#user_msg += \"Be careful to support large number sizes\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# apply hints\n",
        "#system_message += algoritm_hint\n",
        "#user_msg += algoritm_hint\n",
        "\n",
        "print (system_message)\n",
        "print (user_msg)\n",
        "print (python)\n"
      ],
      "metadata": {
        "id": "9-oUPlBfslQu"
      },
      "id": "9-oUPlBfslQu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "uCFsZKnaOYOL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uCFsZKnaOYOL",
        "outputId": "7dbad272-62ba-4002-f613-2d2f8f716359"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, I need to reimplement the given Python code into high-performance C++. Let me start by understanding what each part does.\n",
            "\n",
            "First, the Python code defines an LCG (Linear Congruential Generator) using a generator function. Then, it uses this to generate random numbers for the maximum subarray sum problem, applying Kadane's algorithm. Finally, it runs this 20 times with different seeds and sums the results.\n",
            "\n",
            "So, in C++, I'll need to replicate the LCG. But wait, in Python, the modulus is applied after each yield. In C++, I should use 64-bit integers to prevent overflow since the a and c values are large. So, I'll define the LCG state as a uint64_t.\n",
            "\n",
            "Next, the max_subarray_sum function. The Python version uses a nested loop, which is O(n^2). For n=10000, that's 100 million operations. In C++, I can optimize this by using Kadane's algorithm, which is O(n). That's a big improvement.\n",
            "\n",
            "Wait, the Python code uses a nested loop to compute the maximum subarray sum. But Kadane's algorithm is more efficient. So, I'll implement Kadane's algorithm in C++ to reduce the time complexity.\n",
            "\n",
            "For the LCG, I'll create a struct to hold the state, a, c, m, and current value. The generate function will produce the next number.\n",
            "\n",
            "In the max_subarray_sum function, I'll generate the random numbers using the LCG, then apply Kadane's algorithm. But wait, the random numbers in Python are adjusted to min_val and max_val. So, I need to scale the LCG output accordingly. Since the LCG produces a uint32_t, I'll map it to the desired range.\n",
            "\n",
            "In the total_max_subarray_sum, I'll loop 20 times, each time generating a new seed from the LCG, then compute max_subarray_sum for that seed, and accumulate the total.\n",
            "\n",
            "I also need to handle the timing accurately. Using high_resolution_clock in C++ for better precision.\n",
            "\n",
            "Now, about the data types: since the random numbers can be negative, I'll use int64_t for the array. The sum can be large, but with n=10000 and each element up to 10, the maximum sum is 100000, which fits in a 64-bit integer.\n",
            "\n",
            "Putting it all together, I'll structure the code with the LCG struct, the generate function, the max_subarray_sum using Kadane's algorithm, and the total function. I'll also include necessary headers like <vector>, <chrono>, and <cstdint>.\n",
            "\n",
            "I should test if the LCG produces the same sequence as the Python code. For example, with seed=42, a=1664525, c=1013904223, the first generated number should be (1664525 *42 +1013904223) mod 2^32. I can compute that to ensure it's correct.\n",
            "\n",
            "Finally, I'll make sure the timing is measured correctly and output the result with six decimal places.\n",
            "\n",
            "I think that's all. Now, I'll write the C++ code accordingly.\n",
            "</think>\n",
            "\n",
            "```cpp\n",
            "#include <vector>\n",
            "#include <chrono>\n",
            "#include <cstdint>\n",
            "#include <iostream>\n",
            "#include <algorithm>\n",
            "\n",
            "struct LCG {\n",
            "    uint64_t a = 1664525;\n",
            "    uint64_t c = 1013904223;\n",
            "    uint64_t m = 2ULL << 32; // 2^32\n",
            "    uint64_t value;\n",
            "\n",
            "    LCG(uint64_t seed) : value(seed) {}\n",
            "\n",
            "    uint64_t generate() {\n",
            "        value = (a * value + c) % m;\n",
            "        return value;\n",
            "    }\n",
            "};\n",
            "\n",
            "int max_subarray_sum(int n, uint64_t seed, int min_val, int max_val) {\n",
            "    LCG lcg(seed);\n",
            "    std::vector<int64_t> random_numbers(n);\n",
            "    for (int i = 0; i < n; ++i) {\n",
            "        random_numbers[i] = lcg.generate() % (max_val - min_val + 1) + min_val;\n",
            "    }\n",
            "\n",
            "    int64_t max_sum = std::numeric_limits<int64_t>::min();\n",
            "    int64_t current_sum = 0;\n",
            "\n",
            "    for (int i = 0; i < n; ++i) {\n",
            "        current_sum = std::max(random_numbers[i], current_sum + random_numbers[i]);\n",
            "        max_sum = std::max(max_sum, current_sum);\n",
            "    }\n",
            "\n",
            "    return max_sum;\n",
            "}\n",
            "\n",
            "int total_max_subarray_sum(int n, uint64_t initial_seed, int min_val, int max_val) {\n",
            "    LCG master_lcg(initial_seed);\n",
            "    uint64_t seed = master_lcg.generate();\n",
            "    int64_t total_sum = 0;\n",
            "\n",
            "    for (int i = 0; i < 20; ++i) {\n",
            "        uint64_t current_seed = seed;\n",
            "        total_sum += max_subarray_sum(n, current_seed, min_val, max_val);\n",
            "        seed = master_lcg.generate();\n",
            "    }\n",
            "\n",
            "    return total_sum;\n",
            "}\n",
            "\n",
            "int main() {\n",
            "    const int n = 10000;\n",
            "    const uint64_t initial_seed = 42;\n",
            "    const int min_val = -10;\n",
            "    const int max_val = 10;\n",
            "\n",
            "    auto start = std::chrono::high_resolution_clock::now();\n",
            "    int result = total_max_subarray_sum(n, initial_seed, min_val, max_val);\n",
            "    auto end = std::chrono::high_resolution_clock::now();\n",
            "\n",
            "    std::cout << \"Total Maximum Subarray Sum (20 runs): \" << result << std::endl;\n",
            "    std::cout << \"Execution Time: \" << std::chrono::duration_cast<std::chrono::duration<double>>(end - start).count() << \" seconds\" << std::endl;\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def enriched_user_prompt_for(msg, python):\n",
        "    user_prompt = msg\n",
        "    user_prompt += python\n",
        "    return user_prompt\n",
        "\n",
        "\n",
        "response = run(\n",
        "  system_message=system_message,\n",
        "  user_prompt=enriched_user_prompt_for(user_msg, python),\n",
        "  model=model,\n",
        "  tokenizer=tokenizer,\n",
        "  max_new_tokens=6048 #1024\n",
        ")\n",
        "\n",
        "print (response)\n",
        "write_output(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "ymloa4JXa8j-",
      "metadata": {
        "id": "ymloa4JXa8j-"
      },
      "outputs": [],
      "source": [
        "cleanup_model_resources()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d",
      "metadata": {
        "id": "bf8f8018-f64d-425c-a0e1-d7862aa9592d"
      },
      "source": [
        "# Compiling C++ and executing\n",
        "\n",
        "This next cell contains the command to compile a C++ file on my M1 Mac.  \n",
        "It compiles the file `optimized.cpp` into an executable called `optimized`  \n",
        "Then it runs the program called `optimized`\n",
        "\n",
        "In the next lab (day4), a student has contributed a full solution that compiles to efficient code on Mac, PC and Linux!\n",
        "\n",
        "You can wait for this, or you can google (or ask ChatGPT!) for how to do this on your platform, then replace the lines below.\n",
        "If you're not comfortable with this step, you can skip it for sure - I'll show you exactly how it performs on my Mac.\n",
        "\n",
        "\n",
        "OR alternatively: student Sandeep K.G. points out that you can run Python and C++ code online to test it out that way. Thank you Sandeep!  \n",
        "> Not an exact comparison but you can still get the idea of performance difference.\n",
        "> For example here: https://www.programiz.com/cpp-programming/online-compiler/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40",
      "metadata": {
        "id": "4194e40c-04ab-4940-9d64-b4ad37c5bb40"
      },
      "outputs": [],
      "source": [
        "# Compile C++ and run the executable\n",
        "\n",
        "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
        "!./optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab5e4bc-276c-4555-bd4c-12c699d5e899",
      "metadata": {
        "id": "dab5e4bc-276c-4555-bd4c-12c699d5e899"
      },
      "outputs": [],
      "source": [
        "exec(python_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d24ed5-2c15-4f55-80e7-13a3952b3cb8",
      "metadata": {
        "id": "e8d24ed5-2c15-4f55-80e7-13a3952b3cb8"
      },
      "outputs": [],
      "source": [
        "optimize_gpt(python_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b3d073-88a2-40b2-831c-6f0c345c256f",
      "metadata": {
        "id": "e0b3d073-88a2-40b2-831c-6f0c345c256f"
      },
      "outputs": [],
      "source": [
        "# Replace this with the right C++ compile + execute command for your platform\n",
        "\n",
        "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
        "!./optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xk02KOZ82B1x",
      "metadata": {
        "id": "xk02KOZ82B1x"
      },
      "outputs": [],
      "source": [
        "# T4:\n",
        "# For compiling C++ code optimized for a NVIDIA T4 GPU,\n",
        "# you'll want to use NVIDIA's CUDA compiler (nvcc)\n",
        "\n",
        "!nvcc -O3 -std=c++17 -arch=sm_75 -o optimized optimized.cpp\n",
        "!./optimized\n",
        "\n",
        "#Key points about these flags:\n",
        "\n",
        "# -arch=sm_75 is specifically for T4 GPUs (which use the Turing architecture with compute capability 7.5)\n",
        "# -O3 enables high-level optimizations\n",
        "# -std=c++17 enables C++17 features\n",
        "\n",
        "# CPU:\n",
        "# If your code doesn't actually use CUDA/GPU features and\n",
        "# is just running on CPU, you could use g++ instead:\n",
        "#!g++ -O3 -std=c++17 -march=native -mtune=native -o optimized optimized.cpp\n",
        "#!./optimized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9305446-1d0c-4b51-866a-b8c1e299bf5c",
      "metadata": {
        "id": "e9305446-1d0c-4b51-866a-b8c1e299bf5c"
      },
      "outputs": [],
      "source": [
        "optimize_claude(python_hard)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c181036-8193-4fdd-aef3-fc513b218d43",
      "metadata": {
        "id": "0c181036-8193-4fdd-aef3-fc513b218d43"
      },
      "outputs": [],
      "source": [
        "# Replace this with the right C++ compile + execute command for your platform\n",
        "\n",
        "!clang++ -O3 -std=c++17 -march=armv8.3-a -o optimized optimized.cpp\n",
        "!./optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0",
      "metadata": {
        "id": "0be9f47d-5213-4700-b0e2-d444c7c738c0"
      },
      "outputs": [],
      "source": [
        "def stream_gpt(python):\n",
        "    stream = openai.chat.completions.create(model=OPENAI_MODEL, messages=messages_for(python), stream=True)\n",
        "    reply = \"\"\n",
        "    for chunk in stream:\n",
        "        fragment = chunk.choices[0].delta.content or \"\"\n",
        "        reply += fragment\n",
        "        yield reply.replace('```cpp\\n','').replace('```','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8669f56b-8314-4582-a167-78842caea131",
      "metadata": {
        "id": "8669f56b-8314-4582-a167-78842caea131"
      },
      "outputs": [],
      "source": [
        "def stream_claude(python):\n",
        "    result = claude.messages.stream(\n",
        "        model=CLAUDE_MODEL,\n",
        "        max_tokens=2000,\n",
        "        system=system_message,\n",
        "        messages=[{\"role\": \"user\", \"content\": user_prompt_for(python)}],\n",
        "    )\n",
        "    reply = \"\"\n",
        "    with result as stream:\n",
        "        for text in stream.text_stream:\n",
        "            reply += text\n",
        "            yield reply.replace('```cpp\\n','').replace('```','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d",
      "metadata": {
        "id": "2f1ae8f5-16c8-40a0-aa18-63b617df078d"
      },
      "outputs": [],
      "source": [
        "def optimize(python, model):\n",
        "    if model==\"GPT\":\n",
        "        result = stream_gpt(python)\n",
        "    elif model==\"Claude\":\n",
        "        result = stream_claude(python)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown model\")\n",
        "    for stream_so_far in result:\n",
        "        yield stream_so_far"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a",
      "metadata": {
        "id": "f1ddb38e-6b0a-4c37-baa4-ace0b7de887a"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as ui:\n",
        "    with gr.Row():\n",
        "        python = gr.Textbox(label=\"Python code:\", lines=10, value=python_hard)\n",
        "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
        "    with gr.Row():\n",
        "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
        "        convert = gr.Button(\"Convert code\")\n",
        "\n",
        "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19bf2bff-a822-4009-a539-f003b1651383",
      "metadata": {
        "id": "19bf2bff-a822-4009-a539-f003b1651383"
      },
      "outputs": [],
      "source": [
        "def execute_python(code):\n",
        "    try:\n",
        "        output = io.StringIO()\n",
        "        sys.stdout = output\n",
        "        exec(code)\n",
        "    finally:\n",
        "        sys.stdout = sys.__stdout__\n",
        "    return output.getvalue()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f3ab5d-fcfb-4d3f-8728-9cacbf833ea6",
      "metadata": {
        "id": "77f3ab5d-fcfb-4d3f-8728-9cacbf833ea6"
      },
      "outputs": [],
      "source": [
        "# You'll need to change the code in the try block to compile the C++ code for your platform\n",
        "# I pasted this into Claude's chat UI with a request for it to give me a version for an Intel PC,\n",
        "# and it responded with something that looks perfect - you can try a similar approach for your platform.\n",
        "\n",
        "# M1 Mac version to compile and execute optimized C++ code:\n",
        "\n",
        "def execute_cpp(code):\n",
        "        write_output(code)\n",
        "        try:\n",
        "            compile_cmd = [\"clang++\", \"-Ofast\", \"-std=c++17\", \"-march=armv8.5-a\", \"-mtune=apple-m1\", \"-mcpu=apple-m1\", \"-o\", \"optimized\", \"optimized.cpp\"]\n",
        "            compile_result = subprocess.run(compile_cmd, check=True, text=True, capture_output=True)\n",
        "            run_cmd = [\"./optimized\"]\n",
        "            run_result = subprocess.run(run_cmd, check=True, text=True, capture_output=True)\n",
        "            return run_result.stdout\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            return f\"An error occurred:\\n{e.stderr}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442",
      "metadata": {
        "id": "9a2274f1-d03b-42c0-8dcc-4ce159b18442"
      },
      "outputs": [],
      "source": [
        "css = \"\"\"\n",
        ".python {background-color: #306998;}\n",
        ".cpp {background-color: #050;}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1303932-160c-424b-97a8-d28c816721b2",
      "metadata": {
        "id": "f1303932-160c-424b-97a8-d28c816721b2"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks(css=css) as ui:\n",
        "    gr.Markdown(\"## Convert code from Python to C++\")\n",
        "    with gr.Row():\n",
        "        python = gr.Textbox(label=\"Python code:\", value=python_hard, lines=10)\n",
        "        cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
        "    with gr.Row():\n",
        "        model = gr.Dropdown([\"GPT\", \"Claude\"], label=\"Select model\", value=\"GPT\")\n",
        "    with gr.Row():\n",
        "        convert = gr.Button(\"Convert code\")\n",
        "    with gr.Row():\n",
        "        python_run = gr.Button(\"Run Python\")\n",
        "        cpp_run = gr.Button(\"Run C++\")\n",
        "    with gr.Row():\n",
        "        python_out = gr.TextArea(label=\"Python result:\", elem_classes=[\"python\"])\n",
        "        cpp_out = gr.TextArea(label=\"C++ result:\", elem_classes=[\"cpp\"])\n",
        "\n",
        "    convert.click(optimize, inputs=[python, model], outputs=[cpp])\n",
        "    python_run.click(execute_python, inputs=[python], outputs=[python_out])\n",
        "    cpp_run.click(execute_cpp, inputs=[cpp], outputs=[cpp_out])\n",
        "\n",
        "ui.launch(inbrowser=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}